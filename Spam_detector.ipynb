{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spam detection filter using Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several methods of spam detection. However, the most common ones are:\n",
    "\n",
    "1. Content based: This method is usually used to create automatic filtering rules and to classify emails using machine learning approaches, such as Naïve Bayesian classification, Support Vector Machine, K Nearest Neighbor, Neural Networks. This method normally analyses words, the occurrence, and distributions of words and phrases in the content of emails and used then use generated rules to filter the incoming email spams.\n",
    "\n",
    "2. Case based: This method is carried out in two steps. First, both spam and non-spam emails will be extracted. Then, a few steps of preprocessing, such as feature extraction, and selection, grouping of email data, and evaluating the process are going to be executed to transform the email using client interface. At this stage the data is classified into two vectors, thus machine learning can be used to detect the spam.\n",
    "\n",
    "3. Heuristic or Rule based: This method filters spam based on pre-created rule. It evolves a large number of patterns which are usually regular expressions against a chosen message. Hence, any message score that surpasses a specific threshold is filtered as spam.\n",
    "\n",
    "4. Previous likeness based: This approch is based on previous memory, for instance, a training data and its resemblance to the current email. If the email resembels the spam of the training data, then it will be classified as such.This approach uses the k-nearest neighbor (kNN) for filtering spam emails.\n",
    "\n",
    "5. Adaptive spam filtering technique: This technique divids the incoming email corpse into groups based on the likelyhood persontage of similarity.\n",
    "For further reading check: https://www.sciencedirect.com/science/article/pii/S2405844018353404\n",
    "\n",
    "In this project the Natural Language Processing, NLP package of pyspark is emplimented to build a spam detection filter. The data used in this project are collected from volunteer customers in Singapore and from a UK spam reporting site. The classifier used for this purpose is known as Naive Bayes. \n",
    "It is a simple classifier based on Bayes theorm with the assumption of conditional independence between features given the value of class variation. Even though, it seems that Naive Bayes is an overly simplifed classifier, it works very well with real world problems such as spam filtering.\n",
    "For further reference https://scikit-learn.org/stable/modules/naive_bayes.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init('/home/yohannes/spark-2.4.7-bin-hadoop2.7')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%config Completer.use_jedi=False "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('spam_detector').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = spark.read.csv('/home/yohannes/One_Drive/Python-and-Spark-for-Big-Data-master/Spark_for_Machine_Learning/Natural_Language_Processing/smsspamcollection/SMSSpamCollection',inferSchema=True,sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data inspection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----+--------------------+\n",
      "|summary| _c0|                 _c1|\n",
      "+-------+----+--------------------+\n",
      "|  count|5574|                5574|\n",
      "|   mean|null|               645.0|\n",
      "| stddev|null|                 NaN|\n",
      "|    min| ham| &lt;#&gt;  in mc...|\n",
      "|    max|spam|… we r stayin her...|\n",
      "+-------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+\n",
      "| _c0|                 _c1|\n",
      "+----+--------------------+\n",
      "| ham|Go until jurong p...|\n",
      "| ham|Ok lar... Joking ...|\n",
      "|spam|Free entry in 2 a...|\n",
      "| ham|U dun say so earl...|\n",
      "| ham|Nah I don't think...|\n",
      "|spam|FreeMsg Hey there...|\n",
      "| ham|Even my brother i...|\n",
      "| ham|As per your reque...|\n",
      "|spam|WINNER!! As a val...|\n",
      "|spam|Had your mobile 1...|\n",
      "| ham|I'm gonna be home...|\n",
      "|spam|SIX chances to wi...|\n",
      "|spam|URGENT! You have ...|\n",
      "| ham|I've been searchi...|\n",
      "| ham|I HAVE A DATE ON ...|\n",
      "|spam|XXXMobileMovieClu...|\n",
      "| ham|Oh k...i'm watchi...|\n",
      "| ham|Eh u remember how...|\n",
      "| ham|Fine if thats th...|\n",
      "|spam|England v Macedon...|\n",
      "+----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = data.withColumn('length',length(data['_c1']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------+\n",
      "| _c0|                 _c1|length|\n",
      "+----+--------------------+------+\n",
      "| ham|Go until jurong p...|   111|\n",
      "| ham|Ok lar... Joking ...|    29|\n",
      "|spam|Free entry in 2 a...|   155|\n",
      "| ham|U dun say so earl...|    49|\n",
      "| ham|Nah I don't think...|    61|\n",
      "|spam|FreeMsg Hey there...|   147|\n",
      "| ham|Even my brother i...|    77|\n",
      "| ham|As per your reque...|   160|\n",
      "|spam|WINNER!! As a val...|   157|\n",
      "|spam|Had your mobile 1...|   154|\n",
      "| ham|I'm gonna be home...|   109|\n",
      "|spam|SIX chances to wi...|   136|\n",
      "|spam|URGENT! You have ...|   155|\n",
      "| ham|I've been searchi...|   196|\n",
      "| ham|I HAVE A DATE ON ...|    35|\n",
      "|spam|XXXMobileMovieClu...|   149|\n",
      "| ham|Oh k...i'm watchi...|    26|\n",
      "| ham|Eh u remember how...|    81|\n",
      "| ham|Fine if thats th...|    56|\n",
      "|spam|England v Macedon...|   155|\n",
      "+----+--------------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Tokenizer,CountVectorizer,IDF,StringIndexer,StopWordsRemover"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(inputCol='_c1',outputCol='tokenized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "s_removed = StopWordsRemover(inputCol='tokenized',outputCol='sremoved')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = CountVectorizer(inputCol='sremoved',outputCol='vectorized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf = IDF(inputCol='vectorized',outputCol='idf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The following converts the \"spam/ ham\" column to a \"label\" column\n",
    "str_ind = StringIndexer(inputCol='_c0',outputCol='label')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To assemble the features:\n",
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols=['idf','length'],outputCol='features')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creat a pipeline for the above processes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[str_ind,tokenizer,s_removed,cv,tf_idf,assembler])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pipeline.fit(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| _c0|                 _c1|length|label|           tokenized|            sremoved|          vectorized|                 idf|            features|\n",
      "+----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| ham|Go until jurong p...|   111|  0.0|[go, until, juron...|[go, jurong, poin...|(13423,[7,11,31,6...|(13423,[7,11,31,6...|(13424,[7,11,31,6...|\n",
      "| ham|Ok lar... Joking ...|    29|  0.0|[ok, lar..., joki...|[ok, lar..., joki...|(13423,[0,24,297,...|(13423,[0,24,297,...|(13424,[0,24,297,...|\n",
      "|spam|Free entry in 2 a...|   155|  1.0|[free, entry, in,...|[free, entry, 2, ...|(13423,[2,13,19,3...|(13423,[2,13,19,3...|(13424,[2,13,19,3...|\n",
      "| ham|U dun say so earl...|    49|  0.0|[u, dun, say, so,...|[u, dun, say, ear...|(13423,[0,70,80,1...|(13423,[0,70,80,1...|(13424,[0,70,80,1...|\n",
      "| ham|Nah I don't think...|    61|  0.0|[nah, i, don't, t...|[nah, think, goes...|(13423,[36,134,31...|(13423,[36,134,31...|(13424,[36,134,31...|\n",
      "|spam|FreeMsg Hey there...|   147|  1.0|[freemsg, hey, th...|[freemsg, hey, da...|(13423,[10,60,139...|(13423,[10,60,139...|(13424,[10,60,139...|\n",
      "| ham|Even my brother i...|    77|  0.0|[even, my, brothe...|[even, brother, l...|(13423,[10,53,103...|(13423,[10,53,103...|(13424,[10,53,103...|\n",
      "| ham|As per your reque...|   160|  0.0|[as, per, your, r...|[per, request, 'm...|(13423,[125,184,4...|(13423,[125,184,4...|(13424,[125,184,4...|\n",
      "|spam|WINNER!! As a val...|   157|  1.0|[winner!!, as, a,...|[winner!!, valued...|(13423,[1,47,118,...|(13423,[1,47,118,...|(13424,[1,47,118,...|\n",
      "|spam|Had your mobile 1...|   154|  1.0|[had, your, mobil...|[mobile, 11, mont...|(13423,[0,1,13,27...|(13423,[0,1,13,27...|(13424,[0,1,13,27...|\n",
      "| ham|I'm gonna be home...|   109|  0.0|[i'm, gonna, be, ...|[gonna, home, soo...|(13423,[18,43,120...|(13423,[18,43,120...|(13424,[18,43,120...|\n",
      "|spam|SIX chances to wi...|   136|  1.0|[six, chances, to...|[six, chances, wi...|(13423,[8,17,37,8...|(13423,[8,17,37,8...|(13424,[8,17,37,8...|\n",
      "|spam|URGENT! You have ...|   155|  1.0|[urgent!, you, ha...|[urgent!, won, 1,...|(13423,[13,30,47,...|(13423,[13,30,47,...|(13424,[13,30,47,...|\n",
      "| ham|I've been searchi...|   196|  0.0|[i've, been, sear...|[searching, right...|(13423,[39,96,217...|(13423,[39,96,217...|(13424,[39,96,217...|\n",
      "| ham|I HAVE A DATE ON ...|    35|  0.0|[i, have, a, date...|[date, sunday, wi...|(13423,[552,1697,...|(13423,[552,1697,...|(13424,[552,1697,...|\n",
      "|spam|XXXMobileMovieClu...|   149|  1.0|[xxxmobilemoviecl...|[xxxmobilemoviecl...|(13423,[30,109,11...|(13423,[30,109,11...|(13424,[30,109,11...|\n",
      "| ham|Oh k...i'm watchi...|    26|  0.0|[oh, k...i'm, wat...|[oh, k...i'm, wat...|(13423,[82,214,47...|(13423,[82,214,47...|(13424,[82,214,47...|\n",
      "| ham|Eh u remember how...|    81|  0.0|[eh, u, remember,...|[eh, u, remember,...|(13423,[0,2,49,13...|(13423,[0,2,49,13...|(13424,[0,2,49,13...|\n",
      "| ham|Fine if thats th...|    56|  0.0|[fine, if, thats...|[fine, thats, wa...|(13423,[0,74,105,...|(13423,[0,74,105,...|(13424,[0,74,105,...|\n",
      "|spam|England v Macedon...|   155|  1.0|[england, v, mace...|[england, v, mace...|(13423,[4,30,33,5...|(13423,[4,30,33,5...|(13424,[4,30,33,5...|\n",
      "+----+--------------------+------+-----+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = final_data.select(['label','features'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+\n",
      "|label|            features|\n",
      "+-----+--------------------+\n",
      "|  0.0|(13424,[7,11,31,6...|\n",
      "|  0.0|(13424,[0,24,297,...|\n",
      "|  1.0|(13424,[2,13,19,3...|\n",
      "|  0.0|(13424,[0,70,80,1...|\n",
      "|  0.0|(13424,[36,134,31...|\n",
      "|  1.0|(13424,[10,60,139...|\n",
      "|  0.0|(13424,[10,53,103...|\n",
      "|  0.0|(13424,[125,184,4...|\n",
      "|  1.0|(13424,[1,47,118,...|\n",
      "|  1.0|(13424,[0,1,13,27...|\n",
      "|  0.0|(13424,[18,43,120...|\n",
      "|  1.0|(13424,[8,17,37,8...|\n",
      "|  1.0|(13424,[13,30,47,...|\n",
      "|  0.0|(13424,[39,96,217...|\n",
      "|  0.0|(13424,[552,1697,...|\n",
      "|  1.0|(13424,[30,109,11...|\n",
      "|  0.0|(13424,[82,214,47...|\n",
      "|  0.0|(13424,[0,2,49,13...|\n",
      "|  0.0|(13424,[0,74,105,...|\n",
      "|  1.0|(13424,[4,30,33,5...|\n",
      "+-----+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "final_data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the final data into train and test\n",
    "train,test = final_data.randomSplit([0.7,0.3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import NaiveBayes,DecisionTreeClassifier,LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb = NaiveBayes()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.22 s ± 506 ms per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model = nb.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nb.fit(train)\n",
    "result = model.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|label|            features|       rawPrediction|         probability|prediction|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "|  0.0|(13424,[0,1,2,13,...|[-607.91984003746...|[1.0,1.7094832909...|       0.0|\n",
      "|  0.0|(13424,[0,1,4,50,...|[-827.73417010843...|[1.0,6.4669560244...|       0.0|\n",
      "|  0.0|(13424,[0,1,11,32...|[-886.28398662992...|[1.0,1.0309191579...|       0.0|\n",
      "|  0.0|(13424,[0,1,14,31...|[-216.41327117849...|[1.0,1.0678877119...|       0.0|\n",
      "|  0.0|(13424,[0,1,21,27...|[-772.03618477198...|[1.0,5.6469713950...|       0.0|\n",
      "|  0.0|(13424,[0,1,23,63...|[-1301.5735004127...|[1.0,4.4166853327...|       0.0|\n",
      "|  0.0|(13424,[0,1,30,12...|[-599.46157556861...|[1.0,1.6751203222...|       0.0|\n",
      "|  0.0|(13424,[0,1,498,5...|[-317.87710040843...|[0.99999999999869...|       0.0|\n",
      "|  0.0|(13424,[0,1,3657,...|[-128.15470243565...|[0.99997839283287...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,4,6...|[-1278.3066902210...|[1.0,3.1880269393...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,5,6...|[-2569.1159464781...|[1.0,1.3429293548...|       0.0|\n",
      "|  0.0|(13424,[0,2,3,6,9...|[-3299.4972625934...|[1.0,2.3221141031...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,5,1...|[-2499.3004688084...|[1.0,3.3970043854...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,7,2...|[-511.31693711618...|[1.0,1.0566750200...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,8,1...|[-1310.7118796883...|[1.0,4.1084005433...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,8,2...|[-551.45940618239...|[1.0,5.5971302209...|       0.0|\n",
      "|  0.0|(13424,[0,2,4,44,...|[-1917.9684174911...|[1.0,9.4457368696...|       0.0|\n",
      "|  0.0|(13424,[0,2,5,8,4...|[-827.82289214024...|[1.0,3.9564372937...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,11,...|[-742.25507800821...|[1.0,1.9747998380...|       0.0|\n",
      "|  0.0|(13424,[0,2,7,11,...|[-1418.0127082799...|[1.0,3.9230484476...|       0.0|\n",
      "+-----+--------------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Accuracy evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = MulticlassClassificationEvaluator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9164404364769805\n"
     ]
    }
   ],
   "source": [
    "accuracy = acc.evaluate(result)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.8 s ± 2.71 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model2 = dt.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = dt.fit(train)\n",
    "result2 = model2.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9403201104951989\n"
     ]
    }
   ],
   "source": [
    "accuracy2 = acc.evaluate(result2)\n",
    "print(accuracy2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "svc = LinearSVC()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4min 36s ± 2.54 s per loop (mean ± std. dev. of 7 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%timeit model3 = svc.fit(train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model3 = svc.fit(train)\n",
    "result3 = model3.transform(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9728116927261968\n"
     ]
    }
   ],
   "source": [
    "accuracy3 = acc.evaluate(result3)\n",
    "print(accuracy3)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
